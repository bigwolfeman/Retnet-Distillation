"""
Tests for CE Pretrain Mode feature.

This module tests the cross-entropy only pretraining mode which allows
training without a teacher model, using pure cross-entropy loss.

Tests cover:
- Configuration field and CLI argument (T007-T009)
- CE loss computation (T015)
- Teacher client skipping (T016)
- Checkpoint metadata (T017)
- Resume from CE checkpoint to KD mode (T018)
- Legacy checkpoint handling (T022)
- Evaluation without teacher (T023-T024)
- Edge cases (T030-T031)
"""

import pytest
import torch
import torch.nn.functional as F
from unittest.mock import MagicMock, patch
import tempfile
from pathlib import Path

from src.distillation.config import (
    TrainingConfig,
    parse_cli_args,
    create_config_from_args,
)


class TestConfigPretrainCEOnlyField:
    """Tests for T008: Verify config field exists and defaults to False."""

    def test_field_exists(self):
        """pretrain_ce_only field should exist in TrainingConfig."""
        config = TrainingConfig()
        assert hasattr(config, 'pretrain_ce_only')

    def test_default_is_false(self):
        """pretrain_ce_only should default to False."""
        config = TrainingConfig()
        assert config.pretrain_ce_only is False

    def test_can_set_to_true(self):
        """pretrain_ce_only can be set to True."""
        config = TrainingConfig(pretrain_ce_only=True)
        assert config.pretrain_ce_only is True

    def test_field_in_to_dict(self):
        """pretrain_ce_only should be included in to_dict()."""
        config = TrainingConfig(pretrain_ce_only=True)
        config_dict = config.to_dict()
        assert 'pretrain_ce_only' in config_dict
        assert config_dict['pretrain_ce_only'] is True

    def test_field_from_dict(self):
        """pretrain_ce_only should be restored from dict."""
        config = TrainingConfig.from_dict({'pretrain_ce_only': True})
        assert config.pretrain_ce_only is True


class TestCLIPretrainCEOnlyFlag:
    """Tests for T009: Verify CLI flag sets config correctly."""

    def test_cli_flag_recognized(self):
        """--pretrain-ce-only should be a valid CLI argument."""
        with patch('sys.argv', ['train', '--pretrain-ce-only']):
            args = parse_cli_args()
            assert hasattr(args, 'pretrain_ce_only')
            assert args.pretrain_ce_only is True

    def test_cli_flag_default_none(self):
        """Without flag, pretrain_ce_only should be None (not set)."""
        with patch('sys.argv', ['train']):
            args = parse_cli_args()
            # When not specified, action="store_true" with default=None gives None
            assert args.pretrain_ce_only is None

    def test_cli_flag_overrides_yaml_false(self):
        """CLI --pretrain-ce-only should override YAML pretrain_ce_only: false."""
        with patch('sys.argv', ['train', '--pretrain-ce-only']):
            args = parse_cli_args()
            # Mock a config file that has pretrain_ce_only: false
            with patch('src.distillation.config.load_yaml_config') as mock_load:
                mock_load.return_value = {'pretrain_ce_only': False}
                args.config = 'mock.yaml'
                config = create_config_from_args(args)
                assert config.pretrain_ce_only is True


class TestCELossComputation:
    """Tests for T015: Verify CE loss is computed correctly without teacher."""

    def test_ce_loss_matches_pytorch(self):
        """CE loss should match PyTorch's cross_entropy."""
        batch_size = 2
        seq_len = 4
        vocab_size = 100

        # Create random logits and labels
        logits = torch.randn(batch_size, seq_len, vocab_size)
        labels = torch.randint(0, vocab_size, (batch_size, seq_len))

        # Compute expected loss using PyTorch
        expected_loss = F.cross_entropy(
            logits.view(-1, vocab_size),
            labels.view(-1),
            reduction='mean'
        )

        # This is the pattern we use in trainer._train_step()
        computed_loss = F.cross_entropy(
            logits.view(-1, vocab_size),
            labels.view(-1),
            reduction='mean'
        )

        assert torch.allclose(computed_loss, expected_loss)

    def test_ce_loss_ignores_padding(self):
        """CE loss should handle ignore_index for padding tokens."""
        batch_size = 2
        seq_len = 4
        vocab_size = 100
        pad_token_id = 0

        logits = torch.randn(batch_size, seq_len, vocab_size)
        labels = torch.randint(1, vocab_size, (batch_size, seq_len))
        # Add some padding
        labels[0, -2:] = pad_token_id
        labels[1, -1:] = pad_token_id

        # With ignore_index, padding tokens don't contribute to loss
        loss = F.cross_entropy(
            logits.view(-1, vocab_size),
            labels.view(-1),
            reduction='mean',
            ignore_index=pad_token_id
        )

        assert not torch.isnan(loss)
        assert loss.item() > 0


class TestNoTeacherInitialization:
    """Tests for T016: Verify teacher client is None when pretrain_ce_only=True."""

    def test_teacher_client_none_concept(self):
        """
        Conceptual test: When pretrain_ce_only=True, no teacher should be initialized.

        Note: Full integration test requires the actual training script.
        This tests the concept at the config level.
        """
        config = TrainingConfig(pretrain_ce_only=True)
        # The logic in train.py should check this and skip teacher creation
        assert config.pretrain_ce_only is True
        # Teacher settings should still be present (for config file reuse)
        assert hasattr(config, 'teacher_mode')
        assert hasattr(config, 'teacher_model')


class TestCheckpointMetadata:
    """Tests for T017: Verify pretrain_ce_only and tokenizer_name in checkpoint."""

    def test_checkpoint_metadata_structure(self):
        """Checkpoint config should include pretrain_ce_only and tokenizer_name."""
        # Mock the trainer state dict structure
        config = TrainingConfig(pretrain_ce_only=True)
        mock_tokenizer = MagicMock()
        mock_tokenizer.name_or_path = "meta-llama/Llama-3.2-1B-Instruct"

        # This is what get_state_dict() now produces
        checkpoint_config = {
            'training_config': vars(config),
            'student_config': None,
            'pretrain_ce_only': getattr(config, 'pretrain_ce_only', False),
            'tokenizer_name': getattr(mock_tokenizer, 'name_or_path', None),
        }

        assert checkpoint_config['pretrain_ce_only'] is True
        assert checkpoint_config['tokenizer_name'] == "meta-llama/Llama-3.2-1B-Instruct"


class TestResumeKDFromCECheckpoint:
    """Tests for T018: Verify KD can load CE checkpoint."""

    def test_ce_checkpoint_loadable_in_kd_mode(self):
        """CE checkpoint format should be compatible with KD mode loading."""
        # Mock a CE pretrain checkpoint
        ce_checkpoint = {
            'global_step': 1000,
            'epoch': 1,
            'best_val_loss': 5.0,
            'model_state_dict': {},
            'optimizer_state_dict': {},
            'scaler_state_dict': {},
            'metrics': {},
            'config': {
                'training_config': {'pretrain_ce_only': True},
                'student_config': None,
                'pretrain_ce_only': True,
                'tokenizer_name': 'meta-llama/Llama-3.2-1B-Instruct',
            },
            'rng_states': {},
        }

        # All required keys should be present for KD mode
        required_keys = ['global_step', 'epoch', 'best_val_loss',
                         'model_state_dict', 'optimizer_state_dict']
        for key in required_keys:
            assert key in ce_checkpoint


class TestLegacyCheckpointNoMetadata:
    """Tests for T022: Verify graceful handling of checkpoints without pretrain_ce_only."""

    def test_legacy_checkpoint_defaults_to_false(self):
        """Legacy checkpoints without pretrain_ce_only should default to False."""
        legacy_checkpoint = {
            'global_step': 500,
            'epoch': 0,
            'config': {
                'training_config': {},
                'student_config': None,
                # No pretrain_ce_only field
            },
        }

        checkpoint_config = legacy_checkpoint.get('config', {})
        ce_mode = checkpoint_config.get('pretrain_ce_only', False)
        assert ce_mode is False

    def test_legacy_checkpoint_no_tokenizer_name(self):
        """Legacy checkpoints without tokenizer_name should default to None."""
        legacy_checkpoint = {
            'config': {
                # No tokenizer_name field
            },
        }

        checkpoint_config = legacy_checkpoint.get('config', {})
        tokenizer_name = checkpoint_config.get('tokenizer_name', None)
        assert tokenizer_name is None


class TestEvaluationWithoutTeacher:
    """Tests for T023: Verify perplexity works without teacher."""

    def test_perplexity_computation_concept(self):
        """
        Conceptual test: Perplexity can be computed from CE loss alone.

        Perplexity = exp(CE_loss)
        """
        # Typical CE loss values
        ce_loss = torch.tensor(2.5)
        perplexity = torch.exp(ce_loss)

        assert perplexity > 0
        assert not torch.isnan(perplexity)
        # exp(2.5) â‰ˆ 12.18
        assert 10 < perplexity.item() < 15


class TestNIAHSkippedInCEMode:
    """Tests for T024: Verify NIAH is skipped with log message."""

    def test_niah_requires_teacher_concept(self):
        """
        Conceptual test: NIAH evaluation requires teacher logits.

        When pretrain_ce_only=True, NIAH should be skipped.
        """
        config = TrainingConfig(
            pretrain_ce_only=True,
            eval_niah=True,  # User wants NIAH
        )

        # Even if eval_niah=True, when pretrain_ce_only=True,
        # the training script should skip NIAH and log a message
        assert config.pretrain_ce_only is True
        assert config.eval_niah is True  # Config accepts it, but runtime skips


class TestConfigBothYAMLAndCLI:
    """Tests for T030: Verify redundant setting (YAML + CLI both true) works."""

    def test_both_yaml_and_cli_true(self):
        """Setting pretrain_ce_only in both YAML and CLI should work."""
        with patch('sys.argv', ['train', '--pretrain-ce-only']):
            args = parse_cli_args()
            with patch('src.distillation.config.load_yaml_config') as mock_load:
                mock_load.return_value = {'pretrain_ce_only': True}
                args.config = 'mock.yaml'
                config = create_config_from_args(args)
                assert config.pretrain_ce_only is True


class TestResumeCEFromKDCheckpoint:
    """Tests for T031: Verify CE mode can load KD checkpoint."""

    def test_kd_checkpoint_loadable_in_ce_mode(self):
        """KD checkpoint format should be compatible with CE mode loading."""
        # Mock a KD checkpoint
        kd_checkpoint = {
            'global_step': 2000,
            'epoch': 2,
            'best_val_loss': 4.0,
            'model_state_dict': {},
            'optimizer_state_dict': {},
            'scaler_state_dict': {},
            'metrics': {},
            'config': {
                'training_config': {'pretrain_ce_only': False},
                'student_config': None,
                'pretrain_ce_only': False,
                'tokenizer_name': 'meta-llama/Llama-3.2-1B-Instruct',
            },
            'rng_states': {},
        }

        # All required keys for CE mode should be present
        # (CE mode doesn't need any additional keys)
        required_keys = ['global_step', 'epoch', 'model_state_dict', 'optimizer_state_dict']
        for key in required_keys:
            assert key in kd_checkpoint


if __name__ == '__main__':
    pytest.main([__file__, '-v'])

