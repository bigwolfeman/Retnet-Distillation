# Configuration for Direct + Cache Mode
#
# Use case: Want fast training AND build cache for later experiments
# Loads teacher in VRAM, generates logits, AND saves to disk
#
# Benefits:
#   - Fast training (local inference)
#   - Build cache for future runs
#   - Can resume with cached mode if interrupted
#
# Command:
#   python -m src.distillation.scripts.train --config configs/train_direct_cached.yaml

# Model configuration
model_variant: "500M"
max_seq_length: 4096

# Training hyperparameters
max_steps: 60000
physical_batch_size: 1
gradient_accumulation_steps: 256

# Optimizer settings
learning_rate: 3.0e-4
weight_decay: 0.1
max_grad_norm: 1.0
use_8bit_optimizer: true

# Scheduler settings
warmup_batches: 200  # Warmup over 200 batches (~1 minute)
scheduler_type: "cosine_warmup"
cosine_t0: 10000
cosine_tmult: 2
min_lr: 3.0e-5

# Mixed precision
use_bf16: true
gradient_checkpointing: false

# Teacher settings - DIRECT + CACHE MODE
teacher_mode: "direct"  # Load teacher in memory
teacher_model: "meta-llama/Llama-3.2-1B-Instruct"
teacher_topk: 128
teacher_temperature: 2.0
cache_logits: true  # CACHE WHILE TRAINING
cache_dir: "data/teacher_cache/"  # Where to save cache

# Distillation loss
distill_alpha: 0.2

# Data configuration
train_data_path: "data/train"
val_data_path: "data/val"
tokenizer_name: "meta-llama/Llama-3.2-1B"
num_workers: 4

# Checkpointing
output_dir: "runs/direct_cached_mode"
save_interval: 5000
keep_last_n: 3
max_total_size_gb: 100.0

# Evaluation
eval_interval: 5000
eval_perplexity: true
eval_niah: true
eval_perplexity_samples: 1000
eval_niah_samples: 100

# Telemetry
log_interval: 10
enable_wandb: true
wandb_project: "distillation-llama-retnet"
wandb_run_name: "direct-cached-mode"

# Misc
seed: 42
device: "cuda"
