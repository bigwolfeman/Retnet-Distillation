# Configuration for Network Mode (vLLM server)
#
# Use case: Teacher on remote server, student trains locally
# Runs vLLM server on one machine, training on another
#
# Setup:
#   1. Start vLLM server on GPU machine:
#      vllm serve meta-llama/Llama-3.2-1B-Instruct \
#        --host 0.0.0.0 \
#        --port 8080 \
#        --gpu-memory-utilization 0.9
#
#   2. Train on local machine:
#      python -m src.distillation.scripts.train --config configs/train_network.yaml
#
# Benefits:
#   - Can use any size teacher (not limited by local VRAM)
#   - Share teacher across multiple training jobs
#   - Flexible deployment (cloud, cluster, etc.)
#
# Command:
#   python -m src.distillation.scripts.train --config configs/train_network.yaml

# Model configuration
model_variant: "500M"
max_seq_length: 4096

# Training hyperparameters
max_steps: 60000
physical_batch_size: 1
gradient_accumulation_steps: 256

# Optimizer settings
learning_rate: 3.0e-4
weight_decay: 0.1
max_grad_norm: 1.0
use_8bit_optimizer: true

# Scheduler settings
warmup_batches: 200  # Warmup over 200 batches (~1 minute)
scheduler_type: "cosine_warmup"
cosine_t0: 10000
cosine_tmult: 2
min_lr: 3.0e-5

# Mixed precision
use_bf16: true
gradient_checkpointing: false

# Teacher settings - NETWORK MODE
teacher_mode: "network"  # Query remote vLLM server
teacher_url: "http://localhost:8080"  # vLLM server URL (update with your server IP)
teacher_model: "meta-llama/Llama-3.2-1B-Instruct"
teacher_topk: 128
teacher_temperature: 2.0
teacher_timeout: 30.0  # Request timeout
teacher_max_retries: 3  # Retry on network errors
cache_logits: false  # Set to true to cache network logits

# Distillation loss
distill_alpha: 0.2

# Data configuration
train_data_path: "data/train"
val_data_path: "data/val"
tokenizer_name: "meta-llama/Llama-3.2-1B"
num_workers: 4

# Checkpointing
output_dir: "runs/network_mode"
save_interval: 5000
keep_last_n: 3
max_total_size_gb: 100.0

# Evaluation
eval_interval: 5000
eval_perplexity: true
eval_niah: true
eval_perplexity_samples: 1000
eval_niah_samples: 100

# Telemetry
log_interval: 10
enable_wandb: true
wandb_project: "distillation-llama-retnet"
wandb_run_name: "network-mode"

# Misc
seed: 42
device: "cuda"
