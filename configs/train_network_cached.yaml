# Configuration for Network + Cache Mode
#
# Use case: Use remote vLLM server AND build cache for later
# Queries network for logits and saves to disk
#
# Benefits:
#   - Can use large teacher on remote server
#   - Build cache for future local experiments
#   - Fallback option if network slow/unreliable
#
# Command:
#   python -m src.distillation.scripts.train --config configs/train_network_cached.yaml

# Model configuration
model_variant: "500M"
max_seq_length: 4096

# Training hyperparameters
max_steps: 60000
physical_batch_size: 1
gradient_accumulation_steps: 256

# Optimizer settings
learning_rate: 3.0e-4
weight_decay: 0.1
max_grad_norm: 1.0
use_8bit_optimizer: true

# Scheduler settings
warmup_batches: 200  # Warmup over 200 batches (~1 minute)
scheduler_type: "cosine_warmup"
cosine_t0: 10000
cosine_tmult: 2
min_lr: 3.0e-5

# Mixed precision
use_bf16: true
gradient_checkpointing: false

# Teacher settings - NETWORK + CACHE MODE
teacher_mode: "network"  # Query remote vLLM server
teacher_url: "http://localhost:8080"  # Update with your server IP
teacher_model: "meta-llama/Llama-3.2-1B-Instruct"
teacher_topk: 128
teacher_temperature: 2.0
teacher_timeout: 30.0
teacher_max_retries: 3
cache_logits: true  # CACHE NETWORK LOGITS
cache_dir: "data/teacher_cache_network/"

# Distillation loss
distill_alpha: 0.2

# Data configuration
train_data_path: "data/train"
val_data_path: "data/val"
tokenizer_name: "meta-llama/Llama-3.2-1B"
num_workers: 4

# Checkpointing
output_dir: "runs/network_cached_mode"
save_interval: 5000
keep_last_n: 3
max_total_size_gb: 100.0

# Evaluation
eval_interval: 5000
eval_perplexity: true
eval_niah: true
eval_perplexity_samples: 1000
eval_niah_samples: 100

# Telemetry
log_interval: 10
enable_wandb: true
wandb_project: "distillation-llama-retnet"
wandb_run_name: "network-cached-mode"

# Misc
seed: 42
device: "cuda"
