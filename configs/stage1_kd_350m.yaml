# Stage 1 Knowledge Distillation - 350M Student Model
# Configuration for distilling Llama-3.2-1B teacher to RetNet 350M student

config_version: "1.0.0"
variant_name: "stage1_kd_350m"

# Model configuration
model_variant: "350M"
model_config_path: "configs/model_350m.yaml"

# Training hyperparameters
max_steps: 60000  # Target: 60k-80k steps
physical_batch_size: 1  # Per-device batch size (for 32GB VRAM)
gradient_accumulation_steps: 256  # Effective batch = 256
max_seq_length: 4096  # 4k context for v1

# Optimizer settings
learning_rate: 3.0e-4  # Base learning rate
weight_decay: 0.1  # Weight decay coefficient
max_grad_norm: 1.0  # Gradient clipping
use_8bit_optimizer: true  # Use bitsandbytes AdamW8bit

# Scheduler settings
warmup_batches: 200  # Warmup over 200 batches (~1 minute)
scheduler_type: "cosine_warmup"
cosine_t0: 10000  # First cycle length
cosine_tmult: 2  # Cycle multiplier
min_lr: 3.0e-5  # Minimum learning rate

# Mixed precision
use_bf16: true  # BF16 mixed precision
gradient_checkpointing: false  # Disabled for 350M (fits in VRAM)

# Teacher settings
teacher_device: "cuda"  # Device for teacher model (e.g., "cuda", "cuda:0", "cuda:1")
                        # Use different device than student for memory isolation when using DirectTeacherClient
                        # Reduces 8-10GB memory conflicts on same GPU
teacher_url: "http://localhost:8080"  # vLLM server URL (update with your server IP)
teacher_model: "meta-llama/Llama-3.2-1B-Instruct"
teacher_topk: 128  # Number of top-k logits
teacher_temperature: 2.0  # Softmax temperature
teacher_timeout: 30.0  # Request timeout (seconds)
teacher_max_retries: 3  # Max retry attempts

# Distillation loss settings
distill_alpha: 0.2  # Hard CE mixing (0.2 * CE_hard + 0.8 * KL_soft)

# Data configuration
train_data_path: "data/distillation_preprocessed"  # Training data directory (pretokenized parquet shards)
val_data_path: "data/val"  # Validation data directory
tokenizer_name: "meta-llama/Llama-3.2-1B"  # Llama tokenizer
num_workers: 4  # Data loader workers

# Pretokenized data settings (FIX: Use real preprocessed data, not synthetic test data)
use_pretokenized_data: true  # Use manifest-based pretokenized dataset
pretokenized_splits: null  # Use all splits (19 datasets, 6.6M samples, 2.9B tokens)
                            # Or specify splits: ["openhermes", "numina_cot", "ultrachat"]

# Checkpointing
output_dir: "runs/stage1_kd_350m"  # Output directory
checkpoint_dir: "runs/stage1_kd_350m/checkpoints"  # Checkpoint directory
save_interval: 5000  # Save every 5k steps
keep_last_n: 3  # Keep last 3 checkpoints
max_total_size_gb: 100.0  # Maximum checkpoint storage (GB)
resume_from: null  # Path to resume checkpoint (null = start fresh)

# Evaluation
eval_interval: 5000  # Evaluate every 5k steps
eval_perplexity: true  # Run perplexity evaluation
eval_niah: true  # Run NIAH evaluation
eval_perplexity_samples: 1000  # Number of samples for perplexity
eval_niah_samples: 100  # Number of samples for NIAH

# Telemetry
log_interval: 10  # Log every 10 steps
enable_wandb: false  # Enable wandb logging
wandb_project: "retnet-distillation"  # Wandb project
wandb_run_name: "stage1_kd_350m"  # Wandb run name
wandb_offline: false  # Wandb offline mode

# Resource limits
max_vram_gb: 30.0  # Target VRAM usage

# Misc
seed: 42  # Random seed
device: "cuda"  # Device (cuda or cpu)

# Quality gates (for reference, not enforced by config)
quality_gates:
  target_perplexity_gap: 1.2  # <= teacher_ppl * 1.2
  target_niah_accuracy: 0.95  # >= 95%
  max_vram_gb: 30.0  # <= 30GB
  min_tokens_per_sec: 1000  # >= 1000 tokens/sec

# Training schedule (for reference)
schedule:
  warmup_phase:
    steps: "0-1000"
    description: "Linear warmup from 0 to base LR"

  cycle_1:
    steps: "1000-11000"
    description: "First cosine cycle (10k steps)"

  cycle_2:
    steps: "11000-31000"
    description: "Second cosine cycle (20k steps)"

  cycle_3:
    steps: "31000-71000"
    description: "Third cosine cycle (40k steps)"

  expected_completion:
    steps: "60000-80000"
    description: "Expected completion range"
